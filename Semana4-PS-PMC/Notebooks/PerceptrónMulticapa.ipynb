{"cells":[{"cell_type":"markdown","id":"04259c5e-b23c-4316-b918-fffed2be511b","metadata":{"id":"04259c5e-b23c-4316-b918-fffed2be511b"},"source":["# Perceptrón Multicapa (PMC)\n","La figura a continuación, muestra una red neuronal de 3 capas:\n","- Capa de entrada _(in)_, no se cuenta como capa en sí y corresponde a los datos de entrada\n","- Capa oculta _(h)_\n","- Capa de salida _(o)_\n","\n","<pre>\n","<center><img src=\"https://drive.google.com/uc?export=view&id=1dHD4UEgUOlDIG1gda1HD-YaaQxjzyVD9\" width=700></center>\n","</pre>\n","\n","Llamamos a la salida del _i-ésimo_ nodo en la _l-ésima_ capa: $a_i^{(l)}$.\n","\n","$a_0^{(in)}$ y $a_0^{(h)}$ son las **unidades de bias** los cuales igualamos a 1.\n","\n","Cada nodo en la capa $l$ está conectado a todos los nodos en la capa  $l+1$ a través de un coeficiente de la matriz de pesos. Como ejemplo, la conexión entre el _k-ésimo_ nodo en la capa $l$ al _j-ésimo_ nodo en la capa $l+1$ sería $w_{k,j}^{(l)}$. Cuando todas las neuronas en una capa están conectadas a cada neurona de la capa anterior, la capa se denomina _densa_ o _fully connected_.\n","\n","Denominamos $W^{(h)}$ la matriz de pesos que conecta las entradas con la capa oculta y $W^{(out)}$ la matriz de pesos que conecta la capa oculta a la capa de salida.\n","\n","La matriz de pesos $W^{(h)}$ tiene dimensiones $d$ x $m$ donde _d_ es el número de nodos de la capa oculta y _m_ es el número de entradas mas la unidad de bias."]},{"cell_type":"markdown","id":"73c4404e-10c4-44eb-a695-36417036c0c5","metadata":{"id":"73c4404e-10c4-44eb-a695-36417036c0c5"},"source":["## Algoritmo de entrenamiento de un Perceptrón Multicapa\n","\n","Para empezar, vamos a resumir este procedimiento en 3 pasos generales:\n","1. Iniciando en la capa de entrada, propagamos hacia adelante los datos de entrenameinto a través de la red para obtener la salida.\n","2. En base a la salida de la red, calculamos el error que queremos minimizar usando una función de costo.\n","3. Hacemos la retropropagación del error, hallamos su derivada respecto a cada uno de los pesos en la red y actualizamos los pesos.\n","\n","Después de repetir estos pasos y los pesos convergen, hacemos la propagación hacia adelante para obtener la salida de la red y aplicamos una función umbral para obtener la predicción de la etiqueta de clase."]},{"cell_type":"markdown","id":"26ae4f0c-01a0-4aef-9fda-17521631c255","metadata":{"id":"26ae4f0c-01a0-4aef-9fda-17521631c255"},"source":["## Funciones de activación\n","\n","Para que el algoritmo de retropropagación funcione correctamente, es necesario cambiar las funciones de activación de cada perceptrón con la función logística, igual como hicimos con el Adaline.\n","\n","Existen funciones de activación adicionales que funcionan muy bien con el algoritmo:"]},{"cell_type":"markdown","id":"1644cb82-86c5-4b89-b9c6-e3bd9a076f24","metadata":{"id":"1644cb82-86c5-4b89-b9c6-e3bd9a076f24"},"source":["**Función Logística o Sigmoidea:** función continua y diferenciable en forma de 'S'. Sus valores de salida varían entre 0 y 1.\n","\n","$$\\sigma(z) = \\frac{1}{(1 + exp(-z))}$$\n","\n","**Función tangente hiperbólica:** similar a la anterior pero sus valores de salida varían entre -1 y 1.\n","\n","$$\\tanh(z) = 2\\sigma(2z) -1$$\n","\n","**Función unidad lineal rectificada (ReLU)** es continua pero no diferenciable en z=0, su derivada es 0 para $z<0$.\n","\n","$$ReLU(z) = max(0,z)$$"]},{"cell_type":"code","execution_count":null,"id":"b1f0ed85-4cad-4968-b1f0-ad8e46c88649","metadata":{"id":"b1f0ed85-4cad-4968-b1f0-ad8e46c88649"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def sigmoid(z):\n","    return 1 / (1 + np.exp(-z))\n","\n","def relu(z):\n","    return np.maximum(0, z)\n","\n","def derivative(f, z, eps=0.000001):\n","    return (f(z + eps) - f(z - eps))/(2 * eps)\n","\n","z = np.linspace(-5, 5, 200)\n","\n","plt.figure(figsize=(15,6))\n","\n","plt.subplot(121)\n","plt.plot(z, np.sign(z), \"r-\", linewidth=1, label=\"Escalón\")\n","plt.plot(z, sigmoid(z), \"g--\", linewidth=2, label=\"Sigmoidea\")\n","plt.plot(z, np.tanh(z), \"b-\", linewidth=2, label=\"Tanh\")\n","plt.plot(z, relu(z), \"m-.\", linewidth=2, label=\"ReLU\")\n","plt.grid(True)\n","plt.legend(loc=\"center right\", fontsize=14)\n","plt.title(\"Funciones de Activación\", fontsize=14)\n","plt.axis([-5, 5, -1.2, 1.2])\n","\n","plt.subplot(122)\n","plt.plot(z, derivative(np.sign, z), \"r-\", linewidth=1, label=\"Escalón\")\n","plt.plot(0, 0, \"ro\", markersize=5)\n","plt.plot(0, 0, \"rx\", markersize=10)\n","plt.plot(z, derivative(sigmoid, z), \"g--\", linewidth=2, label=\"Sigmoidea\")\n","plt.plot(z, derivative(np.tanh, z), \"b-\", linewidth=2, label=\"Tanh\")\n","plt.plot(z, derivative(relu, z), \"m-.\", linewidth=2, label=\"ReLU\")\n","plt.grid(True)\n","plt.legend(loc=\"center right\", fontsize=14)\n","plt.title(\"Derivadas\", fontsize=14)\n","plt.axis([-5, 5, -0.2, 1.2])\n","plt.show()"]},{"cell_type":"markdown","id":"27409a00-9ba8-40bc-8cfd-14765a309610","metadata":{"id":"27409a00-9ba8-40bc-8cfd-14765a309610"},"source":["### Uso de la clase _MLPClassifier_ de sklearn\n","\n","A continuación vamos a utilizar la clase _MLPClassifier_ para resolver el problema del XOR.\n","\n","Los datos de entrada (X) y salida (y) se encuentran en el archivo `datos_xor.mat` en la carpeta \"data\""]},{"cell_type":"code","execution_count":null,"id":"28b4a8a4-7e83-4854-8ff2-914ae22e4c77","metadata":{"id":"28b4a8a4-7e83-4854-8ff2-914ae22e4c77"},"outputs":[],"source":["import numpy as np\n","import scipy.io as sio\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","id":"47a48df5","metadata":{"id":"47a48df5"},"source":["**cargamos y graficamos los datos**"]},{"cell_type":"code","execution_count":null,"id":"539d9f82-aaac-41f4-adc7-dccbdeb8138e","metadata":{"id":"539d9f82-aaac-41f4-adc7-dccbdeb8138e"},"outputs":[],"source":["# Cargamos los datos en variables de entrada (X) y salida (y)\n","#----------------------------------------\n"]},{"cell_type":"code","execution_count":null,"id":"9c3630f3-8cc1-43c1-9777-305f8a378b9a","metadata":{"id":"9c3630f3-8cc1-43c1-9777-305f8a378b9a"},"outputs":[],"source":["#Modificamos las etiquetas de clase para tener clases 0 y 1\n","#-------------------------------------------------------\n"]},{"cell_type":"code","execution_count":null,"id":"365bfe73-0009-4e97-82dd-188487e042ae","metadata":{"id":"365bfe73-0009-4e97-82dd-188487e042ae"},"outputs":[],"source":["# Grafiquemos los datos usando plt.scatter\n","#-----------------------------------------------------------------------------\n"]},{"cell_type":"markdown","id":"ce795d4d","metadata":{"id":"ce795d4d"},"source":["Entrenamos un clasificador _MLPClassifier_ con una capa oculta de 2 neuronas, con función de activación sigmoidea, usando el algoritmo de optimización `adam` y un coeficiente de aprendizaje `0.2` y graficar la curva de error. Ver la [documentación](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) de la clase."]},{"cell_type":"code","execution_count":null,"id":"b3ec059b","metadata":{"id":"b3ec059b"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"c5b2d36e-f777-4258-a071-c92958df384e","metadata":{"id":"c5b2d36e-f777-4258-a071-c92958df384e"},"source":["Como vimos en clase de teoría. Las neuronas en la capa oculta, estarían realizando una transformación. Es decir, mapean el espacio de entrada (X) a un nuevo espacio (X') donde los patrones pueden ser linealmente separados por el nodo en la capa de salida.\n","<pre>\n","<center><img src=\"https://drive.google.com/uc?export=view&id=1dGTYgeMbSbARS9i9___OxsHlibS5ErzQ\" width=700></center>\n","</pre>\n","\n","Con los pesos $w_h$ podemos obtener dos rectas que separan la representación de los datos entrada.\n","Podemos obtener las rectas a la salida de la capa oculta a partir de los pesos $w_h$ de la capa, de forma similar como hacíamos con el perceptrón simple:\n","$$\\begin{pmatrix}1\\\\x_1\\\\x_2\\end{pmatrix} \\begin{pmatrix}w_{00}&w_{01}&w_{02}\\\\ w_{10}&w_{11}&w_{12}\\\\ \\end{pmatrix} = 0$$\n","\n","Esto nos dará como resultado dos rectas, de las cuales despejamos la pendiente y ordenada para graficarlas\n","$$ w_{00} + x_1 w_{01} + x_2 w_{02} = 0$$\n","$$ w_{10} + x_1 w_{11} + x_2 w_{12} = 0$$"]},{"cell_type":"markdown","id":"c356342d","metadata":{"id":"c356342d"},"source":["### Obtener los coeficientes de los pesos de la capa oculta y la capa de salida para graficar las rectas"]},{"cell_type":"code","execution_count":null,"id":"e38f0d18","metadata":{"id":"e38f0d18"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"451361a1","metadata":{"id":"451361a1"},"outputs":[],"source":["# Obtenemos los parámetros de las rectas que se forman con los pesos w_h de la capa oculta\n","#---------------------------------------------------------------------------\n","\n","\n","\n","# Graficamos las 2 rectas junto a los datos\n","#------------------------------------------------------------------------------------\n","\n"]},{"cell_type":"markdown","id":"55ad8c16-562e-440c-b420-9a767b59a05f","metadata":{"id":"55ad8c16-562e-440c-b420-9a767b59a05f"},"source":["### Visualizando el espacio transformado\n","El espacio transformado corresponde a la salida de la capa oculta con los pesos entrenados de la red. Esta salida $\\vec{a}^{(h)}$ se obtiene aplicando la función de activación $\\phi()$ a la entrada de la capa oculta $\\vec{z}^{(h)}$\n","\n","$$ \\vec{z}^{(h)} = \\vec{x}^{(in)}.(\\vec{W}^{(h)})^T $$\n","\n","$$ \\vec{a}^{(h)} = \\phi(\\vec{z}^{(h)})$$\n","\n","$\\vec{x}^{(in)}$ corresponde a los datos de entrada X mas el bias"]},{"cell_type":"code","execution_count":null,"id":"dbd214b1","metadata":{"id":"dbd214b1"},"outputs":[],"source":["def sigmoid(z):\n","    return 1 / (1 + np.exp(-z))"]},{"cell_type":"code","execution_count":null,"id":"90a56825","metadata":{"id":"90a56825"},"outputs":[],"source":["# Agrego la columna para bias a la entrada X\n","\n","\n","#Calculamos la salida de la capa oculta\n"]},{"cell_type":"code","execution_count":null,"id":"7b17694a","metadata":{"id":"7b17694a"},"outputs":[],"source":["# Ahora vamos a representar la salida de la capa oculta\n"]},{"cell_type":"markdown","id":"2432e18d-6d54-4399-8f2a-760ce6451c8e","metadata":{"id":"2432e18d-6d54-4399-8f2a-760ce6451c8e"},"source":["### Como se puede observar, ahora los patrones de entrada a la neurona de la capa de salida, son linealmente separables\n","Ahora la frontera de decisión podemos encontrarla de la misma forma que hicimos con el perceptrón simple, excepto que esta vez trabajamos con un espacio _X'_ resultado de la transformación:\n","\n","$$ w_0^{(o)} + x_1' w_1^{(o)} + x_2' w_2^{(o)} = 0$$\n","\n","$w^{(o)}$ corresponde a los pesos en el nodo de salida"]},{"cell_type":"code","execution_count":null,"id":"743a17d5","metadata":{"id":"743a17d5"},"outputs":[],"source":["# Con los pesos del nodo en la capa de salida w_out, armar la recta que separe\n","# los datos en el campo transformado\n","# --------------------------------------------------------------------------------------------------------\n","\n","\n","# Graficar los datos y la recta\n","#-----------------------------------------------------------------------\n","\n"]},{"cell_type":"markdown","source":["Graficar utilizando la función plot_decision_regions"],"metadata":{"id":"0nqxvLd1YqIp"},"id":"0nqxvLd1YqIp"},{"cell_type":"code","execution_count":null,"id":"f0e4c661","metadata":{"id":"f0e4c661"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}