{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "416df03c-eded-499e-a5ae-32ce26c34591",
   "metadata": {},
   "source": [
    "# Ensamble de Clasificadores\n",
    "\n",
    "La idea clave en los ensambles es lograr **diversidad** en los clasificadores del conjunto. Esta diversidad puede lograrse de distintas maneras. \n",
    "El método más común consiste en usar distintos conjuntos de entrenamiento para entrenar clasificadores individuales. Estos conjuntos se pueden obtener mediante muestreo aleatorio de los datos de entrenamiento.\n",
    "\n",
    "Todos los ensambles deben tener 2 componentes principales: Un algoritmo para generar clasificadores diversos y un método para combinar las salidas de los clasificadores. El método más comun para combinar las salidas es la **votación por mayoría**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2e055b-41f5-4570-a0dd-48c5ddb1e8a9",
   "metadata": {},
   "source": [
    "## Bagging (Bootstrap aggregation)\n",
    "En esta estrategia, la diversidad del ensamble se obtiene mediante subconjuntos muestreados de forma aleatoria del conjunto de datos original con reemplazo (pueden repetirse los datos). Estos subconjuntos, se utilizan para entrenar diferentes clasificadores del mismo tipo. La predicción final, se obtiene a partir de las predicciones individuales utilizando en general **_votación por mayoría_**.\n",
    "<pre>\n",
    "<center><img src=\"https://drive.google.com/uc?export=view&id=1dVQmvpA65hbJEgHmnkCcmgskd98PEqaR\" width=600></center>\n",
    "</pre>\n",
    "El proceso de muestreo de los datos con reemplazo se denomina **Bootstrap**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b41828",
   "metadata": {},
   "source": [
    "> Cuando el muestreo se realiza con reemplazo, el método se denomina **Bagging** (boostrap aggregating). Cuando el muestreo se hace sin reemplazo se denomina **Pasting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89231c84-a331-4ee2-89c4-811ed2e89533",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utiles import plot_decision_regions\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Creamos datos \n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bab59e-150c-4604-aa21-d38aa93a08d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos los datos\n",
    "markers = ('o', '^', 'v')\n",
    "colors = ('salmon','skyblue','red', 'gray', 'cyan')\n",
    "plt.figure(figsize=(8,6))\n",
    "for idx, cl in enumerate(np.unique(y)):\n",
    "    plt.scatter(x=X[y == cl, 0], \n",
    "                y=X[y == cl, 1],\n",
    "                alpha=0.7, \n",
    "                c=colors[idx],\n",
    "                marker=markers[idx], \n",
    "                label=cl, \n",
    "                edgecolor='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b8e5bf-b74c-48c1-b24c-2bab58a24070",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier(random_state=0)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "# Graficación\n",
    "plt.figure(figsize=(8,6))\n",
    "plot_decision_regions(tree_clf, X_train, y_train, )\n",
    "plt.xlim(-2,2)\n",
    "plt.ylim(-2,2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589a8787-368d-45be-8591-4970ecac1ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tree_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123cd222-c467-4694-8ada-3fe1f23c950c",
   "metadata": {},
   "source": [
    "**Vamos a comparar el entrenamiento de un árbol de decisión con un ensamble de árboles**\n",
    "\n",
    "**Armamos un ensamble combinando 500 árboles de decisión**\n",
    "\n",
    "Creamos el ensamble usando la clase **_BaggingClassifier_** implementado en el módulo scikit-Learn. Esta es una API para bagging y pasting en función de cómo se seleccionan los parámetros:\n",
    "\n",
    "1. Cuando los subconjuntos se seleccionan de forma aleatoria particionando el dataset sin reemplazo, el algoritmo se conoce como **_Pasting_**\n",
    "2. Cuando los subconjuntos se seleccionan de forma aleatoria particionando el dataset **_con_** reemplazo, el algoritmo se conoce como **_Bagging_**\n",
    "3. Cuando los subconjuntos tienen diferentes características seleccionadas de forma aleatoria, el algoritmo se conoce como **_Random Subspaces_**\n",
    "4. Cuando los subconjuntos combinan los dos anteriores, el algoritmo se conoce como **_Random Patches_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563d7a87-ef84-44b8-ad91-694f50cdb3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(estimator= DecisionTreeClassifier(), \n",
    "                            n_estimators=500, \n",
    "                            max_samples=100, \n",
    "                            max_features = 1.0,\n",
    "                            bootstrap = True,\n",
    "                            n_jobs=-1, \n",
    "                            random_state=0)\n",
    "bag_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61144f3-986b-4113-907a-c20398c48e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(bag_clf, X_train, y_train)\n",
    "plt.xlim(-2,2)\n",
    "plt.ylim(-2,2)\n",
    "plt.title(\"Bagging\")\n",
    "plt.ylabel(\"\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde9583c-3186-409d-b493-c1770ff5fac3",
   "metadata": {},
   "source": [
    "### Evaluación Out of bag (OOB)\n",
    "\n",
    "Es una medida del desempeño aplicada a modelos que utilizan la técnica de **_Bootstrapping_**. En la selección de los datos para cada clasificador, durante su entrenamiento, alrededor del 36% de los datos no son muestreados. El desempeño OOB, representa el promedio de los desempeños de cada clasificador del ensamble cuando se tienen en cuenta estos datos que cada clasificador no ve durante el entrenamiento. Esta estrategia puede entenderse como una especie de validación cruzada y es efectiva para estimar la generalización del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c62232-9897-4465-ad9b-701f9444ae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(estimator= DecisionTreeClassifier(), \n",
    "                            n_estimators=500, \n",
    "                            max_samples=100, \n",
    "                            max_features = 1.0,\n",
    "                            bootstrap = True,\n",
    "                            oob_score=True,\n",
    "                            n_jobs=-1, \n",
    "                            random_state=0)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418a1b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf.estimators_samples_[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cdd8e5-9e24-4213-9842-501288495c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bag_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae76efcf-508f-420d-a63a-a3dc5fad38a6",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "En esta estrategia, los clasificadores son árboles de decisión y los subconjuntos de entrenamiento se eligen por **_Bootstrapping_** o **_Random Subspacing_**.\n",
    "El número de árboles suele ser grande, generalmente entre 100 a 1000 árboles de decisión.\n",
    "\n",
    "Mediante el bootstrapping se introduce aletoriedad al algoritmo, haciendo que cada árbol se forme de manera ligeramente distinta ya que en la formación de los mismos cada uno parte de una muestra diferente y en cada nodo la selección de características cambia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d67b13-b014-47f4-9955-f9d6f8359849",
   "metadata": {},
   "source": [
    "### Parámetros principales del modelo:\n",
    "\n",
    "Este modelo depende principalmente de 2 parámetros:\n",
    "\n",
    "- Número de árboles de decisión. En la función de scikit-learn por defecto es 100.\n",
    "- Número de características (q) que se seleccionan en cada nodo y que permanece fijo en la formación del árbol. En la función de scikit-learn, toma por defecto $q=\\sqrt{F}$ siendo F el número de características totales en el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27b744e-59d9-4b91-ba30-6bd663085075",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500,\n",
    "                                 bootstrap=True, \n",
    "                                 max_leaf_nodes=16,\n",
    "                                 n_jobs=-1, \n",
    "                                 max_features='sqrt',\n",
    "                                 oob_score=True,\n",
    "                                 random_state=0)\n",
    "rnd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a270b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c73b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'n_estimators': [100, 200, 300, 400, 500],\n",
    "              'max_leaf_nodes': [2, 4, 6, 8, 10, 12, 14, 16],\n",
    "              'max_features': ['sqrt', 'log2', None]\n",
    "              }\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "                           RandomForestClassifier(bootstrap=True, oob_score=True, random_state=0), \n",
    "                           param_grid, \n",
    "                           cv=5, \n",
    "                           scoring='accuracy',\n",
    "                           n_jobs=-1\n",
    "                           )\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d064612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492e3955",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee38d2a-0627-41c7-89ba-053c87067023",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18f9d32-67b7-4e72-a62e-b40f328a8a7d",
   "metadata": {},
   "source": [
    "### Importancia de las características\n",
    "\n",
    "Otra utilidad de los Random Forest es que facilitan la medición de la importancia relativa de cada característica. Esta medición de la importancia de las características puede ser muy útil para comprender cuáles características tienen una mayor influencia en las predicciones del modelo. \n",
    "\n",
    "Scikit-Learn evalúa la importancia de una característica analizando cuánto reducen la impureza los nodos del árbol de decisión que utilizan esa característica en promedio a lo largo de todos los árboles en el Bosque Aleatorio. Se calcula un promedio ponderado, donde el peso de cada nodo se determina por el número de muestras de entrenamiento asociadas a él. En otras palabras, los nodos que tienen un mayor impacto en la clasificación de los datos reciben una importancia más alta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f7e0ce-efe2-4bbb-8d70-040e102f54b7",
   "metadata": {},
   "source": [
    "Scikit-Learn calcula esta puntuación automáticamente para cada característica después del entrenamiento, luego escala los resultados para que la suma de todas las importancias sea igual a 1. Se puede acceder al resultado usando el atributo **feature_importances_**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd94b8e-6b53-447e-a89b-5c84b4806e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris(as_frame=True)\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, random_state=2)\n",
    "rnd_clf.fit(iris.data, iris.target)\n",
    "\n",
    "for score, name in zip(rnd_clf.feature_importances_, iris.data.columns):\n",
    "    print(round(score, 2), name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301aa77b-d346-46b9-a029-1a858e5e2a8c",
   "metadata": {},
   "source": [
    "## Boosting:\n",
    "\n",
    "Boosting se refiere a la forma en como un ensamble puede potenciar (boost) un clasificador 'débil' para obtener uno más robusto.\n",
    "\n",
    "En la estrategia de Bagging, los clasificadores se entrenaban de forma independiente. En Boosting los clasificadores se entrenan en serie (consecutivamente) y la performance del clasificador _k_ influencia en el entrenamiento del _k+1_.\n",
    "\n",
    "La idea general es entrenar clasificadores secuencialmente y que cada clasificador corrija a su clasificador predecesor.\n",
    "\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?export=view&id=1T1vTtnuPG6a_iAjclnVt17tSeikFrNVc\" width = 600></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5408ac4c-50ad-4e20-b9b3-3cdc946b630c",
   "metadata": {},
   "source": [
    "### Adaboost\n",
    "Es el primer algoritmo que se volvió popular en 1997, creado por Freund y Schapire.\n",
    "\n",
    "Como idea clave, el algoritmo se enfoca en las muestras mal clasificadas, de forma tal que estas sean muestreadas (up-sampled) cuando el siguiente clasificador es entrenado. Por lo tanto, los datos de entrenamiento de los clasificadores siguientes están orientados a instancias cada vez más difíciles de clasificar.\n",
    "\n",
    "En el algoritmo, cada instancia del conjunto de entrenamiento tiene un peso $\\frac{1}{m}$, con $m$ igual a la cantidad de instancias. Después del entrenamiento del primer clasificador, se calcula su tasa de error $r_j$: $$r_j=\\frac{\\sum\\limits_{\\begin{matrix}\n",
    "i=1\\\\ y_{p}\\neq y \\end{matrix}}^{m}w^{(i)}}{\\sum\\limits_{i=1}^{m}w^{(i)}}$$\n",
    "\n",
    "Después se calcula el peso para cada clasificador $\\alpha_j$, $\\eta$ es un coeficiente de aprendizaje del algoritmo: \n",
    "\n",
    "$$\\alpha_j = \\eta \\log\\frac{1-r_j}{r_j}$$\n",
    "\n",
    "Con este coeficiente se actualizan los pesos de las instancias mal clasificadas: \n",
    "\n",
    "$$w^{(i)} = w^{(i)} exp(\\alpha_j), \\qquad y_p^{(i)} \\neq y^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0263f3a2-1b80-420e-b1e2-211f1d00f5e9",
   "metadata": {},
   "source": [
    "Adaboost usa un esquema de votación no democrático, denominado _weighted majority voting_: aquellos clasificadores que mostraron buena performance durante su entrenamiento son \"recompensados\" con pesos de votación más altos que los demás."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215954cc-83b7-42ba-a69a-dfe92dec33b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "m = len(X_train)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "learning_rate = 1\n",
    "    \n",
    "pesos_instancias = np.ones(m) / m\n",
    "    \n",
    "for i in range(5):\n",
    "    svm_clf = SVC(C=0.2, gamma=0.6, random_state=42)\n",
    "    svm_clf.fit(X_train, y_train, sample_weight=pesos_instancias * m)\n",
    "    y_pred = svm_clf.predict(X_train)\n",
    "\n",
    "    errores = pesos_instancias[y_pred != y_train].sum()\n",
    "    r = errores / pesos_instancias.sum() \n",
    "    alpha = learning_rate * np.log((1 - r) / r)\n",
    "    \n",
    "    #Actualización de los pesos\n",
    "    pesos_instancias[y_pred != y_train] *= np.exp(alpha)\n",
    "    #Se normalizan los pesos\n",
    "    pesos_instancias /= pesos_instancias.sum() \n",
    "    print(np.unique(pesos_instancias*m))\n",
    "    \n",
    "    plot_decision_regions(svm_clf, X_train, y_train)\n",
    "    plt.title(f\"learning_rate = {learning_rate}\")\n",
    "    \n",
    "    plt.text(-0.75, -0.95, \"1\", fontsize=16)\n",
    "    plt.text(-1.05, -0.95, \"2\", fontsize=16)\n",
    "    plt.text(1.0, -0.95, \"3\", fontsize=16)\n",
    "    plt.text(-1.45, -0.5, \"4\", fontsize=16)\n",
    "    plt.text(1.36,  -0.95, \"5\", fontsize=16)\n",
    "plt.xlim(-1.5,2.5)\n",
    "plt.ylim(-1.5,2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e48ec77-5a25-45fb-be14-c462e705978e",
   "metadata": {},
   "source": [
    "sklearn utiliza una versión de AdaBoost llamada SAMME _(Stagewise Additive Modeling using a Multiclass Exponential loss function)_ Si los clasificadores en el ensamble pueden realizar predicciones mediante probabilidades, sklearn utiliza la variante SAMME.R que tiene en cuenta las probabilidades en lugar de las predicciones y generalmente obtiene mejores desempeños."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267b4bab-a1b3-4b67-a071-43dfb4a896a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier( SVC(C=0.2, gamma=0.6, probability=True, random_state=42), \n",
    "                              n_estimators=100,\n",
    "                              learning_rate=1, \n",
    "                              random_state=42,\n",
    "                              algorithm='SAMME.R'\n",
    "                             )\n",
    "ada_clf.fit(X_train, y_train)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plot_decision_regions(ada_clf, X_train, y_train)\n",
    "plt.xlim(-1.5,2.5)\n",
    "plt.ylim(-1.5,2)\n",
    "plt.show()\n",
    "\n",
    "print(ada_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb7712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {  'n_estimators': [10, 20, 30, 40, 50, 100],\n",
    "                'learning_rate': [0.001, 0.01, 0.1, 1, 10],\n",
    "                'estimator__C': [0.001, 0.01, 0.1, 1, 10],\n",
    "                'estimator__gamma': [0.001, 0.01, 0.1, 1, 10]\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV( AdaBoostClassifier( SVC(probability=True, random_state=42)),\n",
    "                            param_grid, \n",
    "                            cv=5, \n",
    "                            scoring='accuracy',\n",
    "                            n_jobs=-1\n",
    "                           )    \n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52e2bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0617b1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26545855",
   "metadata": {},
   "source": [
    "### Clasificadores por Votación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137793e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', LogisticRegression(random_state=42)),\n",
    "        ('rf', RandomForestClassifier(random_state=42)),\n",
    "        ('svc', SVC(random_state=42))\n",
    "    ]\n",
    ")\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c063768",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, clf in voting_clf.named_estimators_.items():\n",
    "    print(name, \"=\", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa9e5f2",
   "metadata": {},
   "source": [
    "Si queremos usar `soft-voting` (devuelve la etiqueta de clase como el argmax de la suma de las probabilidades predichas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9c8c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf.voting = \"soft\"\n",
    "voting_clf.named_estimators[\"svc\"].probability = True\n",
    "voting_clf.fit(X_train, y_train)\n",
    "voting_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9607b040",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "\n",
    "Es un método para combinar clasificadores con el fin de reducir sus sesgos. Las predicciones de cada clasificador individual se apilan y se utilizan como entrada para entrenar un clasificador final que calcula la predicción. Este clasificador final se entrena mediante validación cruzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ce9bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', LogisticRegression(random_state=42)),\n",
    "        ('rf', RandomForestClassifier(random_state=42)),\n",
    "        ('svc', SVC(probability=True, random_state=42))\n",
    "    ],\n",
    "    final_estimator=RandomForestClassifier(random_state=43),\n",
    "    cv=5\n",
    ")\n",
    "stacking_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9a176b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e786791",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050a4a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_clf = AdaBoostClassifier(\n",
    "    base_estimator=None,  # Utilizaremos los modelos base previamente entrenados\n",
    "    n_estimators=50,       # Número de modelos base a entrenar\n",
    "    learning_rate=1.0,     # Tasa de aprendizaje\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Agregar los modelos base al clasificador AdaBoost\n",
    "adaboost_clf.estimators_ = [tree_clf, lr_clf, mlp_clf]\n",
    "\n",
    "# Entrenar el modelo AdaBoost con los modelos base existentes\n",
    "adaboost_clf.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
